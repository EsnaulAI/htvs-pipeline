=== REPORTE DE AUDITOR√çA HTVS ===


=== 1. ESTRUCTURA DE CARPETAS ===
.
./README.md
./config
./config/config.yaml
./data
./data/databases
./data/databases/swissprot
./data/databases/uniref50
./data/raw
./data/raw/pdb6ocr.ent
./data/raw/pdb7kgy.ent
./data/raw/zinc_download.smi
./data/raw/zinc_library.smi
./environment.yaml
./proyecto_completo.txt
./workflow
./workflow/Snakefile
./workflow/envs
./workflow/envs/pydca.yaml
./workflow/scripts
./workflow/scripts/bridge_update_config.py
./workflow/scripts/fetch_chembl_drugs.py
./workflow/scripts/generate_chimerax_scene.py
./workflow/scripts/get_center.py
./workflow/scripts/module1_analyze_pocket.py
./workflow/scripts/module1_check_druggability.py
./workflow/scripts/module1_coevolution_mi.py
./workflow/scripts/module1_discover_best_target.py
./workflow/scripts/module1_map_entropy.py
./workflow/scripts/module1_network_analysis.py
./workflow/scripts/module1_nma_check.py
./workflow/scripts/module1_parse_and_align.py
./workflow/scripts/module1_prep_structure.py
./workflow/scripts/module1_pydca.py
./workflow/scripts/module1_run_blast.py
./workflow/scripts/module1_run_mmseqs.py
./workflow/scripts/module2_filter_entry.py
./workflow/scripts/module2_prep_3d.py
./workflow/scripts/setup_project_auto.py


=== 2. CONFIGURACI√ìN (config.yaml) ===
project_name: Ghost_Jammer_AdeB
structure:
  pdb_id: 6OCR
  chain_id: B
  target_residue: 89
  clean_pdb: results/module1/target_clean.pdb
evolution:
  n_homologs: 2000
  e_value: 0.001
  msa_file: results/module1/alignment.fasta
  conservation_pdb: results/module1/target_conserved.pdb
chemistry:
  zinc_url: http://files.docking.org/2d/BA/BAAB.smi
  raw_library: data/raw/zinc_library.smi
  filtered_library: results/module2/library_filtered.smi
  prepared_dir: results/module2/ligands_3d
  filters:
    mw_max: 600
    logp_max: 3.5
    must_have_amine: true
docking:
  center_x: -38.15550231933594
  center_y: 0.888374924659729
  center_z: 25.03125
  size_x: 22
  size_y: 22
  size_z: 22


=== 3. ORQUESTADOR (Snakefile) ===
configfile: 'config/config.yaml'

# --- REGLA MAESTRA GLOBAL ---
rule all:
    input:
        "results/module1/candidate_ranking.csv",
        "results/module1/config_updated.flag",
        "results/module1/target_prepared.pdbqt",
        "results/module1/view_scene.cxc",
        "results/module2/preparation_done.flag"

# --- M√ìDULO 1: INTELIGENCIA BIOL√ìGICA ---

rule fetch_clean_pdb:
    output:
        pdb=config["structure"]["clean_pdb"],
        fasta="results/module1/target.fasta"
    params:
        pdb_id=config["structure"]["pdb_id"],
        chain=config["structure"]["chain_id"]
    script:
        'scripts/module1_prep_structure.py'

# [cite_start]MEJORA: Aumentamos a 2000 hits y usamos base de datos 'nr' [cite: 4]
rule run_blast:
    input: 
        fasta="results/module1/target.fasta"
    output: 
        xml="results/module1/blast_results.xml"
    params:
        n_hits=2000,       # Aumentado para potencia estad√≠stica en DCA
        e_val=0.001,
        db="nr"            # Base de datos masiva (Non-Redundant)
    script: 
        'scripts/module1_run_blast.py'

rule parse_and_align:
    input: 
        xml="results/module1/blast_results.xml",
        original_fasta="results/module1/target.fasta"
    output: 
        fasta_msa=config["evolution"]["msa_file"]
    threads: 4
    script:
        'scripts/module1_parse_and_align.py'

rule map_conservation:
    input:
        pdb=config["structure"]["clean_pdb"],
        msa=config["evolution"]["msa_file"]
    output: pdb_conserved=config["evolution"]["conservation_pdb"]
    script: 'scripts/module1_map_entropy.py'

rule run_pydca:
    input: msa=config["evolution"]["msa_file"]
    output:
        plot="results/module1/dca_distribution.png",
        report="results/module1/dca_top_contacts.csv"
    params: target_res=config["structure"]["target_residue"]
    script: 'scripts/module1_pydca.py'

rule run_network_analysis:
    input: pdb=config["evolution"]["conservation_pdb"]
    output: report="results/module1/network_report.txt"
    params:
        target_res=config["structure"]["target_residue"],
        chain=config["structure"]["chain_id"]
    script: 'scripts/module1_network_analysis.py'

rule run_nma:
    input: pdb=config["evolution"]["conservation_pdb"]
    output: plot="results/module1/nma_mobility_profile.png"
    params:
        target_res=config["structure"]["target_residue"],
        chain=config["structure"]["chain_id"]
    script: 'scripts/module1_nma_check.py'

rule check_druggability:
    input: pdb=config["structure"]["clean_pdb"]
    output: report="results/module1/druggability_report.txt"
    params:
        chain=config["structure"]["chain_id"],
        target_res=config["structure"]["target_residue"]
    script: 'scripts/module1_check_druggability.py'

rule run_fpocket:
    input: pdb=config["structure"]["clean_pdb"]
    output: info="results/module1/pockets/target_clean_info.txt"
    shell:
        """
        fpocket -f {input.pdb}
        mkdir -p results/module1/pockets
        mv results/module1/target_clean_out/target_clean_info.txt {output.info}
        rm -rf results/module1/target_clean_out
        """

rule prepare_receptor_phys:
    input: pdb=config["structure"]["clean_pdb"]
    output: pdbqt="results/module1/target_prepared.pdbqt"
    log: "logs/module1/prepare_receptor_phys.log"
    shell:
        """
        set -euo pipefail
        mkdir -p $(dirname {log})
        tmp_dir=$(mktemp -d)
        cleanup() { rm -rf "$tmp_dir"; }
        trap cleanup EXIT
        tmp_pdb="$tmp_dir/receptor_preprocessed.pdb"
        tmp_mol2="$tmp_dir/receptor_preprocessed.mol2"

        python workflow/scripts/preprocess_receptor.py {input.pdb} "$tmp_pdb" 2> {log}
        obabel "$tmp_pdb" -O "$tmp_mol2" --addtotitle "receptor_prepared" --neutralize -p 7.4 2>> {log}
        obabel "$tmp_mol2" -O {output.pdbqt} -xr --partialcharge gasteiger -p 7.4 --addtotitle "receptor_prepared" 2>> {log}
        """

Nota: los warnings de aromaticidad o tipado en `obabel` pueden ser benignos si el receptor no contiene anillos arom√°ticos relevantes; revisar el `log` antes de descartar el resultado.

rule discover_best_candidate:
    input:
        pdb=config["evolution"]["conservation_pdb"],
        pockets="results/module1/pockets/target_clean_info.txt"
    output: csv="results/module1/candidate_ranking.csv"
    params: chain=config["structure"]["chain_id"]
    script: 'scripts/module1_discover_best_target.py'

rule update_target_config:
    input:
        csv="results/module1/candidate_ranking.csv",
        pdb=config["structure"]["clean_pdb"]
    output: touch("results/module1/config_updated.flag")
    params: chain=config["structure"]["chain_id"]
    script: 'scripts/bridge_update_config.py'

rule generate_viz:
    input: pdb=config["evolution"]["conservation_pdb"]
    output: cxc="results/module1/view_scene.cxc"
    params:
        target_res=config["structure"]["target_residue"],
        chain=config["structure"]["chain_id"]
    script: 'scripts/generate_chimerax_scene.py'

# --- M√ìDULO 2: QUIMIOINFORM√ÅTICA ---

# --- M√ìDULO 2: QUIMIOINFORM√ÅTICA ---

# 1. Miner√≠a de Datos (API ChEMBL) + Inyecci√≥n de Controles
rule fetch_library:
    output: smi=config["chemistry"]["raw_library"]
    script: 'scripts/fetch_chembl_drugs.py'

# 2. Filtrado Farmacol√≥gico (Reglas eNTRy)
rule filter_ligands:
    input: smi=config["chemistry"]["raw_library"]
    output:
        filtered=config["chemistry"]["filtered_library"],
        report="results/module2/filtering_report.csv"
    params:
        mw=config["chemistry"]["filters"]["mw_max"],
        logp=config["chemistry"]["filters"]["logp_max"],
        amine=config["chemistry"]["filters"]["must_have_amine"]
    script: 'scripts/module2_filter_entry.py'

# 3. Preparaci√≥n 3D Paralela
rule prepare_ligands_3d:
    input: smi=config["chemistry"]["filtered_library"]
    output: touch("results/module2/preparation_done.flag")
    params: out_dir=config["chemistry"]["prepared_dir"]
    threads: 4
    script: 'scripts/module2_prep_3d.py'

=== 4. SCRIPTS (workflow/scripts/*.py) ===

>>> CONTENIDO DE: workflow/scripts/bridge_update_config.py
# --- MOCK PARA DESARROLLO (Pylance no se quejar√°) ---
if "snakemake" not in globals():
    # Esto simula la variable snakemake para que el editor no marque error
    # y t√∫ sepas qu√© estructura tiene. Solo sirve mientras editas.
    from types import SimpleNamespace
    snakemake = SimpleNamespace(
        input=SimpleNamespace(pdb="", fasta="", xml="", original_fasta="", msa=""),
        output=SimpleNamespace(pdb="", fasta="", xml="", fasta_msa="", pdb_conserved=""),
        params=SimpleNamespace(pdb_id="7KGY", chain="B", n_hits=10, e_val=0.001),
        wildcards=SimpleNamespace()
    )
# ----------------------------------------------------
import pandas as pd
import yaml
import numpy as np
from Bio.PDB import PDBParser
import sys

# Inputs desde Snakemake
ranking_csv = snakemake.input.csv
pdb_file = snakemake.input.pdb
config_file = "config/config.yaml"

print("üåâ PUENTE DE DATOS: Actualizando configuraci√≥n con el mejor blanco descubierto...")

# 1. Leer el Ganador
try:
    df = pd.read_csv(ranking_csv)
    # El mejor es la primera fila (ya est√° ordenado por Ghost_Score)
    winner = df.iloc[0]
    best_res_id = int(winner['ID'])
    best_score = winner['Ghost_Score']
    
    print(f"   üèÜ Ganador Indiscutible: Residuo {winner['Residue']}{best_res_id}")
    print(f"   ‚≠ê Ghost Score: {best_score}")

except Exception as e:
    print(f"‚ùå Error leyendo el ranking: {e}")
    sys.exit(1)

# 2. Calcular Centro Geom√©trico (X, Y, Z) del Ganador
parser = PDBParser(QUIET=True)
structure = parser.get_structure("Target", pdb_file)
chain_id = snakemake.params.chain

target_atoms = []
for model in structure:
    for chain in model:
        if chain.id == chain_id:
            for residue in chain:
                if residue.id[1] == best_res_id:
                    target_atoms = [atom.get_coord() for atom in residue]
                    break

if not target_atoms:
    print(f"‚ùå Error: El residuo {best_res_id} no tiene √°tomos en el PDB.")
    sys.exit(1)

center = np.mean(target_atoms, axis=0)
cx, cy, cz = float(center[0]), float(center[1]), float(center[2])

print(f"   üìç Coordenadas calculadas: [{cx:.2f}, {cy:.2f}, {cz:.2f}]")

# 3. Reescribir config.yaml
with open(config_file, 'r') as f:
    config = yaml.safe_load(f)

# Actualizamos valores
config['structure']['target_residue'] = best_res_id
config['docking']['center_x'] = cx
config['docking']['center_y'] = cy
config['docking']['center_z'] = cz

# Escribimos de vuelta preservando el orden (best effort)
with open(config_file, 'w') as f:
    yaml.dump(config, f, sort_keys=False)

print("‚úÖ Configuraci√≥n actualizada autom√°ticamente. El M√≥dulo 2 apuntar√° al lugar correcto.")
----------------------------------------

>>> CONTENIDO DE: workflow/scripts/fetch_chembl_drugs.py
import requests
import sys
import time

def main():
    output_file = snakemake.output.smi
    
    # URL de la API de ChEMBL (European Bioinformatics Institute)
    base_url = "https://www.ebi.ac.uk/chembl/api/data/molecule"
    
    # Par√°metros: F√°rmacos aprobados (Fase 4), Peso molecular < 600, Formato JSON
    params = {
        'max_phase': 4,
        'molecule_properties__mw_freebase__lte': 600,
        'format': 'json',
        'limit': 1500  # Pedimos 1500 f√°rmacos
    }

    print(f"‚õèÔ∏è  Iniciando miner√≠a de datos en ChEMBL (EBI)...")
    print(f"    Endpoint: {base_url}")
    
    try:
        response = requests.get(base_url, params=params)
        response.raise_for_status()
        data = response.json()
        
        molecules = data.get('molecules', [])
        print(f"‚úÖ Se recibieron {len(molecules)} estructuras de la API.")
        
        # Guardamos en formato SMILES
        with open(output_file, 'w') as f:
            # 1. Primero escribimos tus CONTROLES OBLIGATORIOS (Hardcoded para seguridad)
            f.write("NC(=N)NCCC[C@H](NC(=O)c1ccc2ccccc2c1)C(=O)Nc1ccccc1\tCONTROL_PABN_Inhibitor\n")
            f.write("CN(C)C1=C(O)C(C(N)=O)=C(O)[C@@]2(O)C1=C(O)[C@H]3C(=C2)C(=O)C4=C(C=CC=C4O)[C@@H]3N(C)C\tCONTROL_MINOCYCLINE\n")
            f.write("CN1CCN(CC1)C2=C(F)C=C3C(=C2Cl)N(C=C(C3=O)C(=O)O)C4CC4\tCIPROFLOXACIN\n")
            f.write("CC1(C(N2C(S1)C(C2=O)NC(=O)C(C3=CC=CC=C3)N)C(=O)O)C\tAMPICILLIN\n")
            f.write("CC1=C(C(=O)C2=C(C1=O)O)C(=CC(=C2CC(=C)C)O)O\tDOXYCYCLINE\n")
            
            # 2. Luego escribimos los f√°rmacos minados
            count = 0
            for mol in molecules:
                struct = mol.get('molecule_structures')
                if not struct: continue
                
                smiles = struct.get('canonical_smiles')
                chembl_id = mol.get('molecule_chembl_id')
                name = mol.get('pref_name') or chembl_id
                
                # Limpiar nombre (quitar espacios)
                safe_name = name.replace(" ", "_").replace(";", "")
                
                if smiles:
                    f.write(f"{smiles}\t{safe_name}\n")
                    count += 1
                    
        print(f"üíæ Librer√≠a guardada: 5 Controles + {count} F√°rmacos ChEMBL.")

    except Exception as e:
        print(f"‚ùå Error conectando a ChEMBL: {e}")
        # Fallback de emergencia si la API falla (para no detenerte)
        print("‚ö†Ô∏è Generando librer√≠a m√≠nima de emergencia...")
        with open(output_file, 'w') as f:
            f.write("NC(=N)NCCC[C@H](NC(=O)c1ccc2ccccc2c1)C(=O)Nc1ccccc1\tCONTROL_PABN_Inhibitor\n")
            f.write("CN(C)C1=C(O)C(C(N)=O)=C(O)[C@@]2(O)C1=C(O)[C@H]3C(=C2)C(=O)C4=C(C=CC=C4O)[C@@H]3N(C)C\tCONTROL_MINOCYCLINE\n")
        sys.exit(0) # Salimos con √©xito parcial

if __name__ == "__main__":
    main()
----------------------------------------

>>> CONTENIDO DE: workflow/scripts/generate_chimerax_scene.py
# workflow/scripts/generate_chimerax_scene.py

# --- MOCK PARA DESARROLLO (Pylance no se quejar√°) ---
if "snakemake" not in globals():
    # Esto simula la variable snakemake para que el editor no marque error
    from types import SimpleNamespace
    snakemake = SimpleNamespace(
        input=SimpleNamespace(pdb="results/module1/target_conserved.pdb"),
        output=SimpleNamespace(cxc="results/module1/view_scene.cxc"),
        params=SimpleNamespace(pdb_id="6OCR", chain="B", target_res=513),
        wildcards=SimpleNamespace()
    )
# ----------------------------------------------------

# Inputs desde Snakemake
pdb_file = snakemake.input.pdb
target_res = int(snakemake.params.target_res)
chain_id = snakemake.params.chain
output_cxc = snakemake.output.cxc

print(f"üé® Generando escena de ChimeraX para el residuo {target_res}...")

# Tama√±o de la caja de docking (Debe coincidir con lo que tienes en config.yaml)
BOX_SIZE = 22 

# Contenido del script .cxc
# ChimeraX usa comandos distintos a PyMOL.
# Asumimos que la conservaci√≥n est√° guardada en el B-factor

cxc_content = f"""
# --- Script Autom√°tico de Ghost-Jammer para ChimeraX ---

# 1. Cargar Estructura
open {pdb_file}

# 2. Estilo Visual (Calidad de Publicaci√≥n)
lighting soft
graphics silhouettes true color black width 1.5
style surface

# 3. Colorear por Conservaci√≥n (B-factor)
# Rango 0 (Variable/Azul) a 4.3 (Conservado/Rojo/Maroon)
color byattribute bfactor palette cyan:white:maroon range 0,4.3

# 4. Resaltar el Sitio Alost√©rico (Target)
# Seleccionamos el residuo objetivo
select /{chain_id}:{target_res}

# Mostramos sus √°tomos como esferas doradas
show sel atoms
style sel sphere
color sel gold

# Etiqueta flotante
label sel text "Target {target_res}" height 1.5 color black yoffset 2

# 5. DIBUJAR GRIDBOX (Visualizaci√≥n de la Caja de Docking)
# Crea un cubo de malla amarilla centrado exactamente en el residuo seleccionado
shape box center sel size {BOX_SIZE},{BOX_SIZE},{BOX_SIZE} color yellow mesh true name gridbox

# 6. Enfoque final
view orient
zoom sel 0.8
"""

with open(output_cxc, "w") as f:
    f.write(cxc_content)

print(f"‚úÖ Script guardado en: {output_cxc}")
print("   -> Para ver: Abre ChimeraX y ejecuta 'open results/module1/view_scene.cxc'")
----------------------------------------

>>> CONTENIDO DE: workflow/scripts/get_center.py
# workflow/scripts/get_center.py

# --- MOCK PARA DESARROLLO ---
if "snakemake" not in globals():
    from types import SimpleNamespace
    snakemake = SimpleNamespace(
        input=SimpleNamespace(pdb="results/module1/target_conserved.pdb"),
        output=SimpleNamespace(coords="results/module1/gridbox_center.txt"),
        params=SimpleNamespace(chain="B", target_res=513)
    )
# ----------------------------

import numpy as np
from Bio.PDB import PDBParser
import sys

pdb_file = snakemake.input.pdb
chain_id = snakemake.params.chain
target_res_num = int(snakemake.params.target_res)
output_file = snakemake.output.coords

print(f"üìç Calculando centroide geom√©trico para {chain_id}:{target_res_num}...")

parser = PDBParser(QUIET=True)
structure = parser.get_structure('target', pdb_file)

target_residue = None
found = False

# Buscar el residuo
for model in structure:
    for chain in model:
        if chain.id == chain_id:
            for residue in chain:
                if residue.id[1] == target_res_num:
                    target_residue = residue
                    found = True
                    break
        if found: break
    if found: break

if not target_residue:
    print(f"‚ùå Error: Residuo {target_res_num} no encontrado en cadena {chain_id}.")
    sys.exit(1)

# Calcular centroide
coords = []
for atom in target_residue:
    coords.append(atom.get_coord())

center = np.mean(coords, axis=0)
x, y, z = center[0], center[1], center[2]

print(f"‚úÖ Centroide calculado: [{x:.3f}, {y:.3f}, {z:.3f}]")

# Guardar
with open(output_file, "w") as f:
    f.write(f"center_x = {x:.3f}\n")
    f.write(f"center_y = {y:.3f}\n")
    f.write(f"center_z = {z:.3f}\n")
----------------------------------------

>>> CONTENIDO DE: workflow/scripts/module1_analyze_pocket.py
# --- MOCK PARA DESARROLLO (Pylance no se quejar√°) ---
if "snakemake" not in globals():
    from types import SimpleNamespace
    snakemake = SimpleNamespace(
        input=SimpleNamespace(msa="results/module1/alignment.fasta"),
        output=SimpleNamespace(plot="results/module1/dca_distribution.png", report="results/module1/dca_top_contacts.csv"),
        params=SimpleNamespace(pdb_id="7KGY", chain="B", n_hits=10, e_val=0.001, target_res=513),
        wildcards=SimpleNamespace(),
        threads=4
    )
# ----------------------------------------------------
# workflow/scripts/module1_analyze_pocket.py

from Bio.PDB import PDBParser, NeighborSearch, Selection
import numpy as np
import sys

# Inputs
pdb_file = snakemake.input.pdb
chain_id = snakemake.params.chain
target_res_num = int(snakemake.params.target_res)
output_report = snakemake.output.report

print(f"üèòÔ∏è Analizando el vecindario evolutivo alrededor de {chain_id}:{target_res_num}...")

parser = PDBParser(QUIET=True)
structure = parser.get_structure("Target", pdb_file)

# 1. Obtener √°tomos de la cadena correcta
atom_list = [atom for atom in structure[0][chain_id].get_atoms()]
ns = NeighborSearch(atom_list)

# 2. Encontrar el residuo central
target_residue = None
for res in structure[0][chain_id]:
    if res.id[1] == target_res_num:
        target_residue = res
        break

if not target_residue:
    print("‚ùå Error: Target no encontrado.")
    sys.exit(1)

# Usamos el Carbono Alpha o un √°tomo central para buscar vecinos
center_atom = target_residue['CA'] if 'CA' in target_residue else target_residue.child_list[0]

# 3. Buscar vecinos a 6 Angstroms (Radio de contacto de un f√°rmaco)
neighbors = ns.search(center_atom.get_coord(), 6.0, level='R')

# 4. Analizar Conservaci√≥n (Extra√≠da del B-factor)
pocket_scores = []
print("\nüìã RESIDUOS DEL BOLSILLO:")
print(f"{'Residuo':<10} {'Conservaci√≥n (0-4.3)':<20} {'Estado'}")
print("-" * 45)

with open(output_report, "w") as f:
    f.write("Residue,Conservation_Score,Status\n")
    
    for res in neighbors:
        # El score se guard√≥ en el b_factor en el paso map_conservation
        # Tomamos el promedio de los √°tomos del residuo (deber√≠a ser igual para todos)
        score = res['CA'].get_bfactor() if 'CA' in res else list(res.get_atoms())[0].get_bfactor()
        pocket_scores.append(score)
        
        status = "Variable ‚ö†Ô∏è"
        if score > 2.0: status = "Estable"
        if score > 3.5: status = "Cr√≠tico üî•"
        
        line = f"{res.get_resname()}{res.id[1]:<5} {score:<20.2f} {status}"
        print(line)
        f.write(f"{res.get_resname()}{res.id[1]},{score:.2f},{status}\n")

# 5. Veredicto Final
avg_pocket = np.mean(pocket_scores)
print("-" * 45)
print(f"üìä Puntuaci√≥n Global del Bolsillo: {avg_pocket:.2f} / 4.32")

conclusion = ""
if avg_pocket > 3.0:
    conclusion = "‚úÖ BOLSILLO DE ALTA CONFIANZA: Ideal para f√°rmacos de amplio espectro."
elif avg_pocket > 1.5:
    conclusion = "‚ö†Ô∏è BOLSILLO H√çBRIDO: El f√°rmaco debe anclarse a los residuos conservados."
else:
    conclusion = "‚õî BOLSILLO HIPER-VARIABLE: Alto riesgo de resistencia."

print(f"   -> {conclusion}")
with open(output_report, "a") as f:
    f.write(f"\n# GLOBAL SCORE: {avg_pocket:.2f}\n")
    f.write(f"# CONCLUSION: {conclusion}\n")
----------------------------------------

>>> CONTENIDO DE: workflow/scripts/module1_check_druggability.py
# --- MOCK PARA DESARROLLO (Pylance no se quejar√°) ---
if "snakemake" not in globals():
    # Esto simula la variable snakemake para que el editor no marque error
    # y t√∫ sepas qu√© estructura tiene. Solo sirve mientras editas.
    from types import SimpleNamespace
    snakemake = SimpleNamespace(
        input=SimpleNamespace(pdb="", fasta="", xml="", original_fasta="", msa=""),
        output=SimpleNamespace(pdb="", fasta="", xml="", fasta_msa="", pdb_conserved=""),
        params=SimpleNamespace(pdb_id="7KGY", chain="B", n_hits=10, e_val=0.001),
        wildcards=SimpleNamespace()
    )
# ----------------------------------------------------
# workflow/scripts/module1_check_druggability.py

from Bio.PDB import PDBParser, SASA
import numpy as np
import sys

# Inputs
pdb_file = snakemake.input.pdb
chain_id = snakemake.params.chain
target_res_num = int(snakemake.params.target_res)
output_report = snakemake.output.report

print(f"üíß Calculando Farmacabilidad (SASA) para {chain_id}:{target_res_num}...")

try:
    parser = PDBParser(QUIET=True)
    structure = parser.get_structure("Target", pdb_file)
    target_chain = structure[0][chain_id]

    # 1. Calcular SASA (Solvent Accessible Surface Area)
    # Requiere que tengas 'freesasa' o usar el modulo interno de Biopython
    sr = SASA.ShrakeRupley()
    sr.compute(structure, level="R") # Nivel Residuo

    target_sasa = 0
    all_sasa = []

    for residue in target_chain:
        # Biopython agrega el atributo .sasa al residuo
        if hasattr(residue, 'sasa'):
            sasa = residue.sasa
            all_sasa.append(sasa)
            if residue.id[1] == target_res_num:
                target_sasa = sasa

    mean_sasa = np.mean(all_sasa) if all_sasa else 0

    # 2. Diagn√≥stico
    print(f"\nüìä DIAGN√ìSTICO DE ACCESIBILIDAD:")
    print(f"   SASA del Target: {target_sasa:.2f} √Ö¬≤")
    print(f"   Promedio Prote√≠na: {mean_sasa:.2f} √Ö¬≤")

    conclusion = ""
    if target_sasa < 5:
        conclusion = "‚õî BURIED (Enterrado): Sitio profundo. Requiere din√°mica para abrirse."
    elif 5 <= target_sasa <= 30:
        conclusion = "‚úÖ POCKET (Bolsillo): Accesibilidad ideal para sitio cr√≠ptico (Cryptic Pocket)."
    else:
        conclusion = "‚ö†Ô∏è EXPOSED (Expuesto): Superficie muy abierta. Dif√≠cil selectividad."

    print(f"   -> {conclusion}")

    with open(output_report, "w") as f:
        f.write(f"Target Residue: {target_res_num}\n")
        f.write(f"SASA_Score: {target_sasa:.2f}\n")
        f.write(f"Global_Mean_SASA: {mean_sasa:.2f}\n")
        f.write(f"Conclusion: {conclusion}\n")

except Exception as e:
    print(f"‚ùå Error calculando SASA: {e}")
    # Generar reporte de error para no romper el pipeline
    with open(output_report, "w") as f:
        f.write("Error calculation SASA\n")
----------------------------------------

>>> CONTENIDO DE: workflow/scripts/module1_coevolution_mi.py
import numpy as np
import matplotlib.pyplot as plt
from Bio import AlignIO
from collections import Counter
import sys

# Inputs
msa_file = "results/module1/alignment.fasta"
target_res_index = 513 - 1 # Ajuste de √≠ndice (PDB 513 -> Array 512 si empieza en 1, pero depende del mapeo. Usaremos aproximado)
# NOTA: En un pipeline real estricto, debemos alinear √≠ndice PDB <-> √≠ndice MSA. 
# Aqu√≠ asumiremos que el MSA mantiene la numeraci√≥n aproximada tras recortar gaps principales.

print(f"üß¨ Iniciando An√°lisis de Co-evoluci√≥n (Mutual Information) para columna ~{target_res_index}...")

# 1. Cargar MSA
alignment = AlignIO.read(msa_file, "fasta")
num_seqs = len(alignment)
aln_len = alignment.get_alignment_length()

# Convertir a matriz numpy para velocidad
# Codificamos AA como enteros
aa_map = {aa: i for i, aa in enumerate("ACDEFGHIKLMNPQRSTVWY-")}
msa_matrix = np.zeros((num_seqs, aln_len), dtype=int)

for i, record in enumerate(alignment):
    # Solo tomamos residuos est√°ndar, otros a gap
    seq_encoded = [aa_map.get(aa, 20) for aa in record.seq]
    msa_matrix[i, :] = seq_encoded

def calc_entropy(col_data):
    counts = np.bincount(col_data, minlength=21)
    probs = counts[counts > 0] / len(col_data)
    return -np.sum(probs * np.log2(probs))

def calc_mi(col_i, col_j):
    # Probabilidad conjunta
    # Un truco r√°pido para pares: col_i * 100 + col_j (hashing simple)
    # Pero para velocidad en python puro, usaremos zip
    pairs = list(zip(col_i, col_j))
    counts = Counter(pairs)
    total = len(pairs)
    mi = 0.0
    for pair, count in counts.items():
        p_xy = count / total
        p_x = np.sum(col_i == pair[0]) / total
        p_y = np.sum(col_j == pair[1]) / total
        mi += p_xy * np.log2(p_xy / (p_x * p_y))
    return mi

# 2. Calcular MI para el Target contra todos
mi_scores = []
target_col = msa_matrix[:, target_res_index] # Asumimos alineaci√≥n directa por ahora

# Entrop√≠a del target (para normalizar)
h_x = calc_entropy(target_col)

print("   Calculando correlaciones (esto toma unos segundos)...")
for j in range(aln_len):
    if j == target_res_index:
        mi_scores.append(0)
        continue
        
    other_col = msa_matrix[:, j]
    mi = calc_mi(target_col, other_col)
    
    # Normalizar (NMI)
    h_y = calc_entropy(other_col)
    if h_x + h_y == 0:
        nmi = 0
    else:
        nmi = (2 * mi) / (h_x + h_y) # Media arm√≥nica simple
        
    mi_scores.append(nmi)

# 3. An√°lisis de Resultados
top_indices = np.argsort(mi_scores)[-5:][::-1] # Top 5
top_scores = [mi_scores[i] for i in top_indices]

print("\n" + "="*40)
print(f"üîó REPORTE DE CO-EVOLUCI√ìN (Socios de GLU {target_res_index + 1})")
print("="*40)
print(f"Top 5 residuos co-evolucionando con el Target:")
for idx, score in zip(top_indices, top_scores):
    # Aqu√≠ el √≠ndice es del MSA, corresponder√≠a mapearlo al PDB en un caso ideal
    print(f"   Columna MSA {idx + 1}: NMI = {score:.4f}")

if max(top_scores) > 0.1:
    print("\n‚úÖ CONCLUSI√ìN: Existen se√±ales de co-evoluci√≥n.")
    print("   El residuo 'habla' con otros sitios a distancia.")
else:
    print("\n‚ö†Ô∏è CONCLUSI√ìN: Baja se√±al de co-evoluci√≥n.")
    print("   El residuo podr√≠a ser independiente o muy conservado (si no cambia, no co-evoluciona).")

# Plot r√°pido
plt.figure(figsize=(10, 4))
plt.plot(mi_scores)
plt.title(f"Perfil de Co-evoluci√≥n para Residuo {target_res_index + 1}")
plt.xlabel("Residuo (Columna MSA)")
plt.ylabel("Informaci√≥n Mutua Normalizada")
plt.savefig("results/module1/coevolution_profile.png")
print("   -> Gr√°fica guardada: results/module1/coevolution_profile.png")
----------------------------------------

>>> CONTENIDO DE: workflow/scripts/module1_discover_best_target.py
# --- MOCK PARA DESARROLLO (Pylance no se quejar√°) ---
if "snakemake" not in globals():
    # Esto simula la variable snakemake para que el editor no marque error
    # y t√∫ sepas qu√© estructura tiene. Solo sirve mientras editas.
    from types import SimpleNamespace
    snakemake = SimpleNamespace(
        input=SimpleNamespace(pdb="", fasta="", xml="", original_fasta="", msa=""),
        output=SimpleNamespace(pdb="", fasta="", xml="", fasta_msa="", pdb_conserved=""),
        params=SimpleNamespace(pdb_id="7KGY", chain="B", n_hits=10, e_val=0.001),
        wildcards=SimpleNamespace()
    )
# ----------------------------------------------------
import pandas as pd
import networkx as nx
from prody import parsePDB, ANM, calcSqFlucts, confProDy
from Bio.PDB import PDBParser, SASA
import numpy as np
import re
import sys

# Inputs
pdb_file = snakemake.input.pdb
pocket_file = snakemake.input.pockets
chain_id = snakemake.params.chain
output_csv = snakemake.output.csv

confProDy(verbosity='none')

def get_pocket_residues(file_path):
    """Parsea el archivo info de fpocket para sacar residuos en cavidades."""
    pocket_res = set()
    try:
        with open(file_path, 'r') as f:
            lines = f.readlines()
            # Buscamos patrones simples de √≠ndices de residuos si existen en el reporte
            # Nota: fpocket info es complejo. Una heur√≠stica simple es confiar 
            # en la combinaci√≥n de SASA y el archivo PDB de pockets si se usara.
            # Aqu√≠ usamos SASA como proxy principal y fpocket como bono si logramos parsearlo.
            pass 
    except:
        pass
    return pocket_res

print(f"üïµÔ∏è Explorando candidatos en cadena {chain_id}...")

# 1. Cargar Estructura
parser = PDBParser(QUIET=True)
structure = parser.get_structure("Target", pdb_file)
chain = structure[0][chain_id]

# 2. Calcular SASA (Accesibilidad) - CR√çTICO PARA NO ELEGIR SITIOS ENTERRADOS
sr = SASA.ShrakeRupley()
sr.compute(structure, level="R")

# 3. Network Analysis
G = nx.Graph()
residues = [r for r in chain if r.id[0] == " "]
for i, r1 in enumerate(residues):
    for j, r2 in enumerate(residues):
        if i >= j: continue
        try:
            if (r1['CA'] - r2['CA']) < 8.0:
                G.add_edge(r1.id[1], r2.id[1])
        except: continue
centrality = nx.betweenness_centrality(G)

# 4. Din√°mica (NMA - Rigidez)
prody_struct = parsePDB(pdb_file)
calphas = prody_struct.select(f'chain {chain_id} and calpha')
anm = ANM('Analysis')
anm.buildHessian(calphas)
anm.calcModes(n_modes=20)
mobility_map = dict(zip(calphas.getResnums(), calcSqFlucts(anm)))

# 5. INTEGRACI√ìN (Ghost-Score v2.0)
data = []

print("   üìä Evaluando residuos...")

for res in residues:
    res_id = res.id[1]
    
    # M√©tricas Base
    cons_score = res['CA'].get_bfactor() if 'CA' in res else 0 # Viene de map_conservation
    cent_score = centrality.get(res_id, 0)
    
    mob = mobility_map.get(res_id, 999)
    rigidity = 1 / mob if mob > 0 else 0
    
    sasa = res.sasa if hasattr(res, 'sasa') else 0
    
    # --- FILTRO DE DRUGGABILITY ---
    pocket_bonus = 0.0
    
    # Rango ideal de SASA para un bolsillo: 10 - 80 Angstroms^2
    if 10.0 <= sasa <= 80.0:
        pocket_bonus = 30.0 # Es accesible y c√≥ncavo
    elif sasa < 5.0:
        pocket_bonus = -100.0 # ENTERRADO (Como el residuo 513 antiguo) - PENALIZAR
    elif sasa > 100.0:
        pocket_bonus = -10.0 # Demasiado expuesto/plano
        
    # F√≥rmula Maestra
    final_score = (cons_score * 3.0) + (cent_score * 50.0) + (rigidity * 1.5) + pocket_bonus
    
    data.append({
        "Residue": res.get_resname(),
        "ID": res_id,
        "Ghost_Score": round(final_score, 3),
        "Conservation": round(cons_score, 2),
        "SASA": round(sasa, 1),
        "Rigidity": round(rigidity, 1)
    })

# Ranking
df = pd.DataFrame(data).sort_values(by="Ghost_Score", ascending=False)
df.to_csv(output_csv, index=False)

winner = df.iloc[0]
print(f"üèÜ GANADOR CIENT√çFICO: {winner['Residue']}{winner['ID']}")
print(f"   Score: {winner['Ghost_Score']} (SASA: {winner['SASA']})")
----------------------------------------

>>> CONTENIDO DE: workflow/scripts/module1_map_entropy.py
# --- MOCK PARA DESARROLLO (Pylance no se quejar√°) ---
if "snakemake" not in globals():
    # Esto simula la variable snakemake para que el editor no marque error
    # y t√∫ sepas qu√© estructura tiene. Solo sirve mientras editas.
    from types import SimpleNamespace
    snakemake = SimpleNamespace(
        input=SimpleNamespace(pdb="", fasta="", xml="", original_fasta="", msa=""),
        output=SimpleNamespace(pdb="", fasta="", xml="", fasta_msa="", pdb_conserved=""),
        params=SimpleNamespace(pdb_id="7KGY", chain="B", n_hits=10, e_val=0.001),
        wildcards=SimpleNamespace()
    )
# ----------------------------------------------------
import math
from Bio import AlignIO
from Bio.PDB import PDBParser, PDBIO

pdb_input = snakemake.input.pdb
msa_input = snakemake.input.msa
pdb_output = snakemake.output.pdb_conserved

def calculate_shannon_entropy(alignment):
    """Calcula la entrop√≠a de Shannon por columna del alineamiento.
    Baja entrop√≠a = Alta conservaci√≥n."""
    n_seqs = len(alignment)
    aln_len = alignment.get_alignment_length()
    entropy_scores = []

    for col_idx in range(aln_len):
        column = alignment[:, col_idx]
        # Contar frecuencias de amino√°cidos (ignorando gaps '-')
        counts = {}
        non_gap_count = 0
        for residue in column:
            if residue == '-' or residue == 'X':
                continue
            counts[residue] = counts.get(residue, 0) + 1
            non_gap_count += 1
        
        if non_gap_count == 0:
            entropy_scores.append(0) # Columna vac√≠a
            continue

        entropy = 0
        for residue in counts:
            p = counts[residue] / non_gap_count
            entropy -= p * math.log2(p)
        
        # Invertir puntuaci√≥n: Queremos que Alto Valor = Alta Conservaci√≥n
        # La entrop√≠a m√°xima para 20 AA es log2(20) ‚âà 4.32
        conservation = 4.32 - entropy
        if conservation < 0: conservation = 0
        entropy_scores.append(conservation)
        
    return entropy_scores

print("üßÆ Calculando conservaci√≥n evolutiva...")
alignment = AlignIO.read(msa_input, "fasta")
scores = calculate_shannon_entropy(alignment)

# Mapear al PDB
parser = PDBParser(QUIET=True)
structure = parser.get_structure("Target", pdb_input)
# Asumimos que la primera secuencia del MSA es nuestra PDB (porque la pusimos primera)
# Necesitamos mapear √≠ndice MSA -> Residuo PDB (saltando gaps en la seq 1)

target_seq_in_msa = alignment[0].seq
pdb_residues = list(structure.get_residues())

msa_index = 0
pdb_index = 0

print("üé® Inyectando puntuaciones en el factor B del PDB...")

for msa_char in target_seq_in_msa:
    score = scores[msa_index]
    
    if msa_char != '-':
        # Si no es un gap en nuestra secuencia, corresponde a un residuo del PDB
        if pdb_index < len(pdb_residues):
            residue = pdb_residues[pdb_index]
            # Asignar score al B-factor de cada √°tomo del residuo
            for atom in residue:
                atom.set_bfactor(score)
            pdb_index += 1
            
    msa_index += 1

# Guardar PDB con datos inyectados
io = PDBIO()
io.set_structure(structure)
io.save(pdb_output)

print(f"‚úÖ ¬°√âxito! Archivo generado: {pdb_output}")
print("   -> Abre este archivo en PyMOL y colorea por B-factor para ver la conservaci√≥n.")
----------------------------------------

>>> CONTENIDO DE: workflow/scripts/module1_network_analysis.py
# --- MOCK PARA DESARROLLO ---
if "snakemake" not in globals():
    from types import SimpleNamespace
    snakemake = SimpleNamespace(
        input=SimpleNamespace(pdb="results/module1/target_conserved.pdb"),
        output=SimpleNamespace(report="results/module1/network_report.txt"),
        params=SimpleNamespace(chain="B", target_res=513)
    )
# ----------------------------

import networkx as nx
import numpy as np
from Bio.PDB import PDBParser
import sys

# Inputs
pdb_file = snakemake.input.pdb
chain_id = snakemake.params.chain

# --- CAMBIO CR√çTICO: Recibir variable desde Config ---
target_residue = int(snakemake.params.target_res)
# ---------------------------------------------------

output_report = snakemake.output.report

print(f"üï∏Ô∏è Iniciando An√°lisis de Redes (Graph Theory) en cadena {chain_id}...")

parser = PDBParser(QUIET=True)
structure = parser.get_structure("Target", pdb_file)
chain = structure[0][chain_id]

# 1. Construir el Grafo (Residue Interaction Network)
G = nx.Graph()
residues = list(chain.get_residues())
residues = [r for r in residues if r.id[0] == " "] # Solo amino√°cidos est√°ndar

# A√±adir nodos
for r in residues:
    G.add_node(r.id[1], resname=r.get_resname())

# A√±adir aristas (Contactos < 8 Angstroms entre Carbonos Alpha)
print("   Calculando contactos f√≠sicos...")
for i, r1 in enumerate(residues):
    for j, r2 in enumerate(residues):
        if i >= j: continue 
        
        try:
            d = r1['CA'] - r2['CA']
            if d < 8.0:
                G.add_edge(r1.id[1], r2.id[1])
        except KeyError:
            continue

print(f"   Grafo construido: {G.number_of_nodes()} nodos, {G.number_of_edges()} conexiones.")

# 2. Calcular Centralidad
print("üßÆ Calculando 'Betweenness Centrality'...")
centrality = nx.betweenness_centrality(G)

# 3. Analizar tu Blanco
my_score = centrality.get(target_residue, 0)
max_score = max(centrality.values()) if centrality else 0
rank = sorted(centrality.values(), reverse=True).index(my_score) + 1 if centrality else 0

# --- Generar Reporte ---
with open(output_report, "w") as f:
    f.write(f"REPORTE DE RED PARA RESIDUO {target_residue}\n")
    f.write("="*40 + "\n")
    f.write(f"Centralidad: {my_score:.4f} (Max: {max_score:.4f})\n")
    f.write(f"Ranking: #{rank} de {len(residues)}\n")
    
    print("\n" + "="*40)
    print(f"üìä REPORTE DE RED PARA GLU {target_residue}")
    print(f"Centralidad: {my_score:.4f} (Max en prote√≠na: {max_score:.4f})")
    print(f"Ranking: #{rank} de {len(residues)} residuos")

    if rank < len(residues) * 0.1:
        msg = "‚úÖ CONCLUSI√ìN: Es un HUB de comunicaci√≥n (Top 10%). Confirmado ALOST√âRICO."
    else:
        msg = "‚ö†Ô∏è CONCLUSI√ìN: No es un hub central estructural."
    
    f.write(msg + "\n")
    print(msg)
----------------------------------------

>>> CONTENIDO DE: workflow/scripts/module1_nma_check.py
# --- MOCK PARA DESARROLLO ---
if "snakemake" not in globals():
    from types import SimpleNamespace
    snakemake = SimpleNamespace(
        input=SimpleNamespace(pdb="results/module1/target_conserved.pdb"),
        output=SimpleNamespace(plot="results/module1/nma_mobility_profile.png"),
        params=SimpleNamespace(chain="B", target_res=513)
    )
# ----------------------------

import sys
import matplotlib.pyplot as plt
import numpy as np

# Importar expl√≠citamente las funciones de ProDy
from prody import parsePDB, ANM, calcSqFlucts, confProDy

# Configuraci√≥n silenciosa
confProDy(verbosity='none')

def run_nma_analysis(pdb_file, chain_id, target_res_num, output_img):
    print(f"üåä Iniciando An√°lisis de Modos Normales (ANM) para {pdb_file}...")
    
    # 1. Cargar estructura
    structure = parsePDB(pdb_file)
    calphas = structure.select(f'chain {chain_id} and calpha')
    
    if calphas is None:
        print(f"‚ùå Error: No se encontraron Carbonos Alpha en la cadena {chain_id}")
        return

    # 2. Construir Modelo El√°stico (ANM)
    anm = ANM('AdeB dynamics')
    anm.buildHessian(calphas)
    anm.calcModes(n_modes=20) 
    
    # 3. Calcular Fluctuaciones
    sq_flucts = calcSqFlucts(anm)
    res_nums = calphas.getResnums()
    
    # Buscar nuestro residuo objetivo
    try:
        target_idx = list(res_nums).index(target_res_num)
        target_fluct = sq_flucts[target_idx]
        print(f"\nüîç AN√ÅLISIS DE MOVILIDAD PARA GLU {target_res_num}:")
        print(f"   Fluctuaci√≥n Cuadr√°tica: {target_fluct:.4f} √Ö¬≤")
        
        mean_fluct = np.mean(sq_flucts)
        if target_fluct < mean_fluct:
            print("   CONCLUSI√ìN: Es una zona R√çGIDA (Posible ancla o bisagra est√°tica).")
        else:
            print("   CONCLUSI√ìN: Es una zona FLEXIBLE (Posible loop o puerta).")
            
    except ValueError:
        print(f"‚ö†Ô∏è El residuo {target_res_num} no est√° en la selecci√≥n analizada.")

    # 4. Generar Gr√°fica
    plt.figure(figsize=(10, 5))
    plt.plot(res_nums, sq_flucts, label='Movilidad Te√≥rica (ANM)', color='black', linewidth=1)
    
    # Marcar nuestro target
    plt.axvline(x=target_res_num, color='red', linestyle='--', label=f'GLU {target_res_num}')
    
    plt.title('Perfil de Flexibilidad de AdeB (An√°lisis de Modos Normales)')
    plt.xlabel('N√∫mero de Residuo')
    plt.ylabel('Fluctuaci√≥n Cuadr√°tica (√Ö¬≤)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Guardar gr√°fica
    plt.savefig(output_img, dpi=300)
    print(f"\nüìä Gr√°fica guardada en: {output_img}")

if __name__ == "__main__":
    # Obtener par√°metros desde Snakemake
    pdb_in = snakemake.input.pdb
    chain_in = snakemake.params.chain
    
    # --- CAMBIO CR√çTICO: Recibir variable desde Config ---
    res_in = int(snakemake.params.target_res)
    # ---------------------------------------------------
    
    out_plot = snakemake.output.plot
    
    run_nma_analysis(pdb_in, chain_in, res_in, out_plot)
----------------------------------------

>>> CONTENIDO DE: workflow/scripts/module1_parse_and_align.py
# --- MOCK PARA DESARROLLO (Pylance no se quejar√°) ---
if "snakemake" not in globals():
    # Esto simula la variable snakemake para que el editor no marque error
    # y t√∫ sepas qu√© estructura tiene. Solo sirve mientras editas.
    from types import SimpleNamespace
    snakemake = SimpleNamespace(
        input=SimpleNamespace(pdb="", fasta="", xml="", original_fasta="", msa=""),
        output=SimpleNamespace(pdb="", fasta="", xml="", fasta_msa="", pdb_conserved=""),
        params=SimpleNamespace(pdb_id="7KGY", chain="B", n_hits=10, e_val=0.001),
        wildcards=SimpleNamespace()
    )
# ----------------------------------------------------
import subprocess
from Bio.Blast import NCBIXML
from Bio import SeqIO
from Bio.Seq import Seq
from Bio.SeqRecord import SeqRecord
import sys

xml_input = snakemake.input.xml
orig_fasta = snakemake.input.original_fasta
msa_output = snakemake.output.fasta_msa
threads = snakemake.threads

print(f"üìñ Leyendo resultados XML y preparando alineamiento...")

hits = []
# Leemos nuestra secuencia original primero
original_rec = SeqIO.read(orig_fasta, "fasta")
hits.append(original_rec)

try:
    with open(xml_input) as result_handle:
        blast_record = NCBIXML.read(result_handle)
        
        for alignment in blast_record.alignments:
            for hsp in alignment.hsps:
                # Filtro de Calidad:
                # El alineamiento debe cubrir al menos el 60% de nuestra prote√≠na
                if hsp.align_length < len(original_rec.seq) * 0.6:
                    continue
                
                # Reconstruimos la secuencia
                seq = Seq(hsp.sbjct)
                rec = SeqRecord(seq, id=alignment.hit_id, description=alignment.hit_def)
                hits.append(rec)
except Exception as e:
    print(f"‚ùå Error leyendo XML: {e}")
    sys.exit(1)

print(f"   -> Se encontraron {len(hits)} secuencias hom√≥logas v√°lidas.")

if len(hits) < 5:
    print("‚ö†Ô∏è ADVERTENCIA CR√çTICA: Muy pocas secuencias para an√°lisis evolutivo.")

# Guardar temporalmente
temp_fasta = "results/module1/unaligned_homologs.fasta"
SeqIO.write(hits, temp_fasta, "fasta")

print(f"üß¨ Ejecutando MAFFT (Alineamiento M√∫ltiple)...")
cmd = f"mafft --auto --quiet --thread {threads} {temp_fasta} > {msa_output}"
subprocess.run(cmd, shell=True, check=True)

print("‚úÖ Alineamiento completado.")
----------------------------------------

>>> CONTENIDO DE: workflow/scripts/module1_prep_structure.py
# --- MOCK PARA DESARROLLO (Pylance no se quejar√°) ---
if "snakemake" not in globals():
    # Esto simula la variable snakemake para que el editor no marque error
    # y t√∫ sepas qu√© estructura tiene. Solo sirve mientras editas.
    from types import SimpleNamespace
    snakemake = SimpleNamespace(
        input=SimpleNamespace(pdb="", fasta="", xml="", original_fasta="", msa=""),
        output=SimpleNamespace(pdb="", fasta="", xml="", fasta_msa="", pdb_conserved=""),
        params=SimpleNamespace(pdb_id="7KGY", chain="B", n_hits=10, e_val=0.001),
        wildcards=SimpleNamespace()
    )
# ----------------------------------------------------
import sys
from Bio.PDB import PDBList, PDBParser, Select, PDBIO, Polypeptide

# Snakemake injects inputs/outputs/params automatically
pdb_id = snakemake.params.pdb_id
chain_target = snakemake.params.chain
out_pdb = snakemake.output.pdb
out_fasta = snakemake.output.fasta

print(f"üî¨ Iniciando descarga y limpieza de {pdb_id} cadena {chain_target}...")

# 1. Descargar
pdbl = PDBList()
pdb_file = pdbl.retrieve_pdb_file(pdb_id, pdir="data/raw", file_format="pdb")

# 2. Parsear y Limpiar
class ChainSelect(Select):
    def accept_chain(self, chain):
        return chain.get_id() == chain_target
    def accept_residue(self, residue):
        # Eliminar aguas (HOH) y heter√°tomos que no sean parte de la prote√≠na
        return residue.id[0] == " "

parser = PDBParser(QUIET=True)
structure = parser.get_structure(pdb_id, pdb_file)

io = PDBIO()
io.set_structure(structure)
io.save(out_pdb, select=ChainSelect())

# 3. Extraer Secuencia FASTA (FIX: Concatenar fragmentos)
ppb = Polypeptide.PPBuilder()
peptides = ppb.build_peptides(structure[0][chain_target])

if len(peptides) == 0:
    print("‚ùå Error Cr√≠tico: No se encontraron p√©ptidos en la cadena especificada.")
    sys.exit(1)

# Aqu√≠ est√° el arreglo: Unimos todos los fragmentos en una sola string
full_seq = "".join([str(pp.get_sequence()) for pp in peptides])

print(f"‚ÑπÔ∏è  Nota: La estructura tiene {len(peptides)} fragmentos. Se han unido para el an√°lisis BLAST.")

with open(out_fasta, "w") as f:
    f.write(f">{pdb_id}_{chain_target}\n{full_seq}\n")

print("‚úÖ Estructura limpia y secuencia unificada extra√≠da.")
----------------------------------------

>>> CONTENIDO DE: workflow/scripts/module1_pydca.py
# --- MOCK PARA DESARROLLO (Pylance no se quejar√°) ---
if "snakemake" not in globals():
    from types import SimpleNamespace
    snakemake = SimpleNamespace(
        input=SimpleNamespace(msa="results/module1/alignment.fasta"),
        output=SimpleNamespace(plot="results/module1/dca_distribution.png", report="results/module1/dca_top_contacts.csv"),
        params=SimpleNamespace(pdb_id="7KGY", chain="B", n_hits=10, e_val=0.001, target_res=513),
        wildcards=SimpleNamespace(),
        threads=4
    )
# ----------------------------------------------------

from pydca.meanfield_dca import meanfield_dca
from Bio import SeqIO
import numpy as np
import matplotlib.pyplot as plt
import sys
import os

msa_file = snakemake.input.msa
output_plot = snakemake.output.plot 
output_report = snakemake.output.report 

# --- CAMBIO CR√çTICO: Recibir variable desde Config ---
target_res_pdb = int(snakemake.params.target_res)
# ---------------------------------------------------

print(f"üß† Iniciando PyDCA (Direct Coupling Analysis) - Campo Medio...")

# 0. VERIFICACI√ìN DE SEGURIDAD (CR√çTICO)
records = list(SeqIO.parse(msa_file, "fasta"))
num_seqs = len(records)
print(f"   üìä Secuencias detectadas para an√°lisis: {num_seqs}")

if num_seqs < 5:
    print("‚ö†Ô∏è  ADVERTENCIA: Insuficientes secuencias para an√°lisis evolutivo (DCA).")
    print("    Se requieren hom√≥logos variados para detectar co-evoluci√≥n.")
    print("    -> Generando archivos vac√≠os para completar el pipeline sin errores.")
    
    with open(output_report, "w") as f:
        f.write("Error,Residue1,Residue2,Score\n")
        f.write(f"SKIPPED,0,0,0.0\n")
        f.write(f"# Reason: Only {num_seqs} sequences found. Need diversity for DCA.\n")
    
    plt.figure()
    plt.text(0.1, 0.5, f"DCA Omitido: Solo {num_seqs} secuencia(s).\nSe requiere mas diversidad.", fontsize=12)
    plt.savefig(output_plot)
    
    sys.exit(0)

# 1. Instanciar DCA
try:
    mfdca = meanfield_dca.MeanFieldDCA(
        msa_file,
        biomolecule="protein", 
        pseudocount=0.5,
        seqid=0.8
    )

    # 2. Calcular Matriz de Acoplamiento
    print("   Calculando matriz inversa (esto consume CPU)...")
    fnapc = mfdca.compute_sorted_FN_APC() 

    # 3. Analizar Resultados
    print(f"   Analizando top interacciones fuertes...")
    top_interactions = fnapc[:20] 

    with open(output_report, "w") as f:
        f.write("Residue1,Residue2,Score_DCA\n")
        found_target = False
        
        print("\nüèÜ Top Conexiones Evolutivas Reales (DCA):")
        for pair in top_interactions:
            # pair es ((i, chain), (j, chain), score)
            res1, res2, score = pair[0][0], pair[1][0], pair[2]
            f.write(f"{res1},{res2},{score}\n")
            print(f"   Res {res1} <--> Res {res2} : Score {score:.4f}")
            
            if abs(res1 - target_res_pdb) < 5 or abs(res2 - target_res_pdb) < 5:
                found_target = True

        if found_target:
            print(f"\n‚úÖ ¬°BINGO! El entorno de GLU {target_res_pdb} aparece en los acoplamientos fuertes.")
        else:
            print(f"\n‚ÑπÔ∏è GLU {target_res_pdb} no est√° en el Top 20 global.")

    # 4. Plot de Contactos
    data = np.array([x[2] for x in fnapc])
    plt.figure()
    plt.plot(data)
    plt.title("Distribuci√≥n de Scores DCA (Ley de Potencia)")
    plt.xlabel("Ranking de Pares")
    plt.ylabel("Fuerza de Acoplamiento")
    plt.savefig(output_plot)
    print(f"   -> Gr√°fica guardada: {output_plot}")

except Exception as e:
    print(f"‚ùå Error cr√≠tico en PyDCA: {e}")
    sys.exit(1)
----------------------------------------

>>> CONTENIDO DE: workflow/scripts/module1_run_blast.py
# --- MOCK PARA DESARROLLO (Pylance no se quejar√°) ---
if "snakemake" not in globals():
    # Esto simula la variable snakemake para que el editor no marque error
    # y t√∫ sepas qu√© estructura tiene. Solo sirve mientras editas.
    from types import SimpleNamespace
    snakemake = SimpleNamespace(
        input=SimpleNamespace(pdb="", fasta="", xml="", original_fasta="", msa=""),
        output=SimpleNamespace(pdb="", fasta="", xml="", fasta_msa="", pdb_conserved=""),
        params=SimpleNamespace(pdb_id="7KGY", chain="B", n_hits=10, e_val=0.001),
        wildcards=SimpleNamespace()
    )
# ----------------------------------------------------
from Bio.Blast import NCBIWWW
from Bio import SeqIO
import sys

# Inputs de Snakemake
fasta_input = snakemake.input.fasta
xml_output = snakemake.output.xml
e_val = snakemake.params.e_val
n_hits = snakemake.params.n_hits
# Default a 'nr' si no se especifica, para m√°xima potencia en PyDCA
db_name = snakemake.params.get("db", "nr") 

print(f"üöÄ Iniciando BLAST Remoto (NCBI) contra '{db_name}' para {fasta_input}...")
print(f"   üéØ Objetivo: {n_hits} secuencias (E-value: {e_val})")
print("   ‚è≥ Esto puede tardar 10-20 minutos. No cierres la terminal...")

try:
    record = SeqIO.read(fasta_input, "fasta")
    
    # Usamos 'nr' (Non-Redundant) para obtener miles de secuencias y alimentar PyDCA
    result_handle = NCBIWWW.qblast(
        "blastp", 
        db_name, 
        record.seq, 
        hitlist_size=n_hits, 
        expect=e_val
    )

    with open(xml_output, "w") as out_handle:
        out_handle.write(result_handle.read())

    result_handle.close()
    print(f"‚úÖ Resultados BLAST guardados en {xml_output}")

except Exception as e:
    print(f"‚ùå Error conectando a BLAST: {e}")
    sys.exit(1)
----------------------------------------

>>> CONTENIDO DE: workflow/scripts/module1_run_mmseqs.py
# --- MOCK PARA DESARROLLO ---
if "snakemake" not in globals():
    from types import SimpleNamespace
    snakemake = SimpleNamespace(
        input=SimpleNamespace(pdb="", fasta="results/module1/target.fasta", xml="", original_fasta="", msa=""),
        output=SimpleNamespace(pdb="", fasta="", xml="", fasta_msa="", fasta_homologs="results/module1/homologs_unaligned.fasta"),
        params=SimpleNamespace(pdb_id="7KGY", chain="B", n_hits=10, e_val=0.001),
        threads=4,
        wildcards=SimpleNamespace()
    )
# ----------------------------

# workflow/scripts/module1_run_mmseqs.py
import subprocess
import os
import sys
import shutil
from Bio import SeqIO

# Inputs/Outputs (Snakemake)
query_fasta = snakemake.input.fasta
out_msa_ready = snakemake.output.fasta_homologs
db_dir = "data/databases/uniref50" 
threads = snakemake.threads

# --- PAR√ÅMETROS AGRESIVOS (Correcci√≥n) ---
# Sensibilidad m√°xima (7.5 es muy alto, encuentra hom√≥logos remotos)
sensitivity = 7.5 
# Permitimos hasta 3000 secuencias para tener estad√≠stica robusta
max_seqs = 3000     
# E-value relajado (permitimos primos lejanos)
e_value = 100.0
# Identidad m√≠nima baj√≠sima (queremos diversidad, no clones)
min_id = 0.05 
# -----------------------------------------

db_target = os.path.join(db_dir, "uniref50_db")
tmp_dir = "results/module1/tmp_mmseqs"

print(f"üöÄ Iniciando B√∫squeda Profunda MMseqs2 (Sensibilidad {sensitivity})...")

if os.path.exists(tmp_dir): shutil.rmtree(tmp_dir)
os.makedirs(tmp_dir, exist_ok=True)
os.makedirs(db_dir, exist_ok=True)

def run_cmd(cmd):
    print(f"   CMD: {cmd}")
    try:
        subprocess.run(cmd, shell=True, check=True)
    except subprocess.CalledProcessError:
        print("‚ùå Error ejecutando MMseqs2.")
        sys.exit(1)

# 1. Base de Datos
if not os.path.exists(db_target + ".dbtype"):
    print("   ‚¨áÔ∏è Descargando UniRef50...")
    run_cmd(f"mmseqs databases UniRef50 {db_target} {tmp_dir}")

# 2. B√∫squeda Iterativa
query_db = os.path.join(tmp_dir, "queryDB")
result_db = os.path.join(tmp_dir, "resultDB")

run_cmd(f"mmseqs createdb {query_fasta} {query_db}")

# Agregamos --start-sens 1 para ir escalando la b√∫squeda
# Agregamos -e {e_value} para capturar todo
cmd_search = (
    f"mmseqs search {query_db} {db_target} {result_db} {tmp_dir} "
    f"-s {sensitivity} --num-iterations 4 --max-seqs {max_seqs} "
    f"--threads {threads} --min-seq-id {min_id} -e {e_value} "
    f"--cov-mode 0 -c 0.3" # Cobertura requerida bajada al 30%
)
run_cmd(cmd_search)

# 3. Extracci√≥n
fasta_res = os.path.join(tmp_dir, "results.fasta")
run_cmd(f"mmseqs result2flat {query_db} {db_target} {result_db} {fasta_res} --use-fasta-header")

# 4. Filtrado Inteligente
print("   QC: Filtrando resultados redundantes o basura...")
original_rec = SeqIO.read(query_fasta, "fasta")
found_recs = list(SeqIO.parse(fasta_res, "fasta"))

unique_recs = []
seen_seqs = set()

# Siempre incluimos el target original primero
unique_recs.append(original_rec)
seen_seqs.add(str(original_rec.seq))

for rec in found_recs:
    s_str = str(rec.seq)
    # Evitar duplicados exactos
    if s_str in seen_seqs: continue
    # Evitar secuencias rotas (muy cortas)
    if len(rec.seq) < len(original_rec.seq) * 0.4: continue
    
    unique_recs.append(rec)
    seen_seqs.add(s_str)

print(f"   üìä Secuencias encontradas: {len(found_recs)}")
print(f"   ‚úÖ Secuencias √∫nicas v√°lidas: {len(unique_recs)}")

if len(unique_recs) < 50:
    print("‚ö†Ô∏è ALERTA: A√∫n tenemos pocas secuencias. La conservaci√≥n ser√° poco confiable.")

SeqIO.write(unique_recs, out_msa_ready, "fasta")
----------------------------------------

>>> CONTENIDO DE: workflow/scripts/module2_filter_entry.py
# --- MOCK PARA DESARROLLO (Pylance no se quejar√°) ---
if "snakemake" not in globals():
    # Esto simula la variable snakemake para que el editor no marque error
    # y t√∫ sepas qu√© estructura tiene. Solo sirve mientras editas.
    from types import SimpleNamespace
    snakemake = SimpleNamespace(
        input=SimpleNamespace(pdb="", fasta="", xml="", original_fasta="", msa=""),
        output=SimpleNamespace(pdb="", fasta="", xml="", fasta_msa="", pdb_conserved=""),
        params=SimpleNamespace(pdb_id="7KGY", chain="B", n_hits=10, e_val=0.001),
        wildcards=SimpleNamespace()
    )
# workflow/scripts/module2_filter_entry.py
from rdkit import Chem
from rdkit.Chem import Descriptors
import pandas as pd
import sys

# Definir patr√≥n de Amina Primaria (Nitr√≥geno con 2 hidr√≥genos, no amida)
PRIMARY_AMINE = Chem.MolFromSmarts("[NX3;H2;!$(NC=O)]")

def passes_entry_rules(mol):
    if mol is None: return False, "Invalid Molecule"
    
    # 1. Peso Molecular (< 600 Da)
    mw = Descriptors.MolWt(mol)
    if mw > 600:
        return False, f"MW High ({mw:.1f})"
        
    # 2. Amina Primaria (Regla clave para Gram-negativas)
    if not mol.HasSubstructMatch(PRIMARY_AMINE):
        return False, "No Primary Amine"
        
    return True, "Pass"

def main():
    input_file = snakemake.input[0]
    output_smi = snakemake.output[0]
    output_csv = snakemake.output[1]
    
    print(f"üß™ Iniciando filtrado eNTRy sobre: {input_file}")
    
    passed_count = 0
    total_count = 0
    report_data = []
    
    # Abrir archivos
    with open(output_smi, 'w') as out_f:
        # Leer SMILES (asumiendo formato: SMILES ID)
        with open(input_file, 'r') as in_f:
            for line in in_f:
                if not line.strip(): continue
                parts = line.split()
                smiles = parts[0]
                mol_id = parts[1] if len(parts) > 1 else f"Ligand_{total_count}"
                
                mol = Chem.MolFromSmiles(smiles)
                total_count += 1
                
                is_valid, reason = passes_entry_rules(mol)
                
                report_data.append({
                    "ID": mol_id,
                    "Status": "Accepted" if is_valid else "Rejected",
                    "Reason": reason,
                    "SMILES": smiles
                })
                
                if is_valid:
                    passed_count += 1
                    out_f.write(f"{smiles}\t{mol_id}\n")

    # Guardar reporte
    df = pd.DataFrame(report_data)
    df.to_csv(output_csv, index=False)
    
    print(f"‚úÖ Filtrado completado: {passed_count}/{total_count} ligandos aceptados.")

if __name__ == "__main__":
    main()# workflow/scripts/module2_filter_entry.py
from rdkit import Chem
from rdkit.Chem import Descriptors
import pandas as pd
import sys

# Definir patr√≥n de Amina Primaria (Nitr√≥geno con 2 hidr√≥genos, no amida)
PRIMARY_AMINE = Chem.MolFromSmarts("[NX3;H2;!$(NC=O)]")

def passes_entry_rules(mol):
    if mol is None: return False, "Invalid Molecule"
    
    # 1. Peso Molecular (< 600 Da)
    mw = Descriptors.MolWt(mol)
    if mw > 600:
        return False, f"MW High ({mw:.1f})"
        
    # 2. Amina Primaria (Regla clave para Gram-negativas)
    if not mol.HasSubstructMatch(PRIMARY_AMINE):
        return False, "No Primary Amine"
        
    return True, "Pass"

def main():
    input_file = snakemake.input[0]
    output_smi = snakemake.output[0]
    output_csv = snakemake.output[1]
    
    print(f"üß™ Iniciando filtrado eNTRy sobre: {input_file}")
    
    passed_count = 0
    total_count = 0
    report_data = []
    
    # Abrir archivos
    with open(output_smi, 'w') as out_f:
        # Leer SMILES (asumiendo formato: SMILES ID)
        with open(input_file, 'r') as in_f:
            for line in in_f:
                if not line.strip(): continue
                parts = line.split()
                smiles = parts[0]
                mol_id = parts[1] if len(parts) > 1 else f"Ligand_{total_count}"
                
                mol = Chem.MolFromSmiles(smiles)
                total_count += 1
                
                is_valid, reason = passes_entry_rules(mol)
                
                report_data.append({
                    "ID": mol_id,
                    "Status": "Accepted" if is_valid else "Rejected",
                    "Reason": reason,
                    "SMILES": smiles
                })
                
                if is_valid:
                    passed_count += 1
                    out_f.write(f"{smiles}\t{mol_id}\n")

    # Guardar reporte
    df = pd.DataFrame(report_data)
    df.to_csv(output_csv, index=False)
    
    print(f"‚úÖ Filtrado completado: {passed_count}/{total_count} ligandos aceptados.")

if __name__ == "__main__":
    main()
----------------------------------------

>>> CONTENIDO DE: workflow/scripts/module2_prep_3d.py
# --- MOCK PARA DESARROLLO (Pylance no se quejar√°) ---
if "snakemake" not in globals():
    # Esto simula la variable snakemake para que el editor no marque error
    # y t√∫ sepas qu√© estructura tiene. Solo sirve mientras editas.
    from types import SimpleNamespace
    snakemake = SimpleNamespace(
        input=SimpleNamespace(pdb="", fasta="", xml="", original_fasta="", msa=""),
        output=SimpleNamespace(pdb="", fasta="", xml="", fasta_msa="", pdb_conserved=""),
        params=SimpleNamespace(pdb_id="7KGY", chain="B", n_hits=10, e_val=0.001),
        wildcards=SimpleNamespace()
    )
# ----------------------------------------------------
# workflow/scripts/module2_prep_3d.py
from rdkit import Chem
from rdkit.Chem import AllChem
from concurrent.futures import ProcessPoolExecutor
import sys
import os

def process_ligand(line):
    parts = line.strip().split()
    if not parts: return None
    
    smi = parts[0]
    # Guardamos el nombre en una variable de Python simple para que no se pierda
    name_str = parts[1] if len(parts) > 1 else "Unknown"
    
    try:
        mol = Chem.MolFromSmiles(smi)
        if mol is None: return None
        
        # 1. Hidr√≥genos a pH 7.4
        mol = Chem.AddHs(mol)
        
        # 2. Generar M√∫ltiples Conf√≥rmeros
        params = AllChem.ETKDGv3()
        params.useSmallRingTorsions = True
        # Intentamos generar 5 conformaciones
        ids = AllChem.EmbedMultipleConfs(mol, numConfs=5, params=params)
        
        if not ids:
            ids = AllChem.EmbedMultipleConfs(mol, numConfs=5, useRandomCoords=True)
            if not ids: return None
            
        # 3. Minimizaci√≥n de Energ√≠a
        best_energy = float('inf')
        best_conf_id = -1
        
        # Optimizar todas
        results = AllChem.MMFFOptimizeMoleculeConfs(mol, numThreads=0)
        
        for i, (converged, energy) in enumerate(results):
            if energy < best_energy:
                best_energy = energy
                best_conf_id = i
                
        if best_conf_id == -1: return None
        
        # 4. Crear la mol√©cula final limpia
        final_mol = Chem.Mol(mol)
        final_mol.RemoveAllConformers()
        final_mol.AddConformer(mol.GetConformer(best_conf_id))
        
        # --- CORRECCI√ìN CR√çTICA: Re-asignar el nombre expl√≠citamente ---
        # Aseguramos que la propiedad _Name viaje con el objeto final
        final_mol.SetProp("_Name", name_str)
        
        return final_mol
    except Exception:
        return None

def main():
    input_smi = snakemake.input.smi
    out_dir = snakemake.params.out_dir
    threads = snakemake.threads
    output_flag = snakemake.output[0] # El archivo .flag

    print(f"‚ö° TURBO V2 (FIXED): Procesando ligandos con {threads} n√∫cleos...")
    os.makedirs(out_dir, exist_ok=True)
    
    with open(input_smi, 'r') as f:
        lines = f.readlines()
    
    count = 0
    
    # Paralelizaci√≥n
    with ProcessPoolExecutor(max_workers=threads) as executor:
        for mol in executor.map(process_ligand, lines):
            if mol:
                # --- BLINDAJE EXTRA: Verificar si trae nombre ---
                if mol.HasProp("_Name"):
                    name = mol.GetProp("_Name")
                else:
                    # Si por alguna raz√≥n se perdi√≥, inventamos uno para no romper el pipeline
                    name = f"Ligand_{count}_recovered"
                
                # Limpiar nombre de caracteres peligrosos para archivos
                safe_name = "".join([c for c in name if c.isalnum() or c in ('_','-')])
                if not safe_name: safe_name = f"Ligand_{count}"
                
                outfile = os.path.join(out_dir, f"{safe_name}.sdf")
                
                w = Chem.SDWriter(outfile)
                w.write(mol)
                w.close()
                count += 1

    # Crear el flag para que Snakemake sepa que terminamos
    with open(output_flag, 'w') as f:
        f.write("Done")

    print(f"‚úÖ Procesamiento Terminado: {count} ligandos listos en {out_dir}")

if __name__ == "__main__":
    main()
----------------------------------------

>>> CONTENIDO DE: workflow/scripts/setup_project_auto.py
import requests
import pandas as pd
import yaml
import sys
import os

# CONFIGURACI√ìN
ORGANISM = "Acinetobacter baumannii"
KEYWORDS = ["AdeB", "Efflux", "Pump", "Transporter"] 
CONFIG_FILE = "config/config.yaml"

print(f"üöÄ BUSCANDO {ORGANISM} (MODO SIN CENSURA)...")

# 1. B√öSQUEDA AMPLIA
query = {
    "query": {
        "type": "terminal",
        "service": "text",
        "parameters": {
            "attribute": "rcsb_entity_source_organism.taxonomy_lineage.name",
            "operator": "contains_phrase",
            "value": ORGANISM
        }
    },
    "return_type": "entry",
    "request_options": {"return_all_hits": True}
}

try:
    resp = requests.post("https://search.rcsb.org/rcsbsearch/v2/query", json=query)
    pdb_ids = [item["identifier"] for item in resp.json().get("result_set", [])]
    
    print(f"   -> {len(pdb_ids)} estructuras totales encontradas.")

    # 2. DESCARGA Y FILTRADO
    gql_query = """
    query structure_info($ids: [String!]!) {
      entries(entry_ids: $ids) {
        rcsb_id
        struct { title }
        rcsb_entry_info {
          resolution_combined
          structure_determination_methodology
          deposition_date
        }
      }
    }
    """
    
    candidates = []
    chunk_size = 50
    data_url = "https://data.rcsb.org/graphql"

    print("   -> Analizando calidad...")
    for i in range(0, len(pdb_ids), chunk_size):
        chunk = pdb_ids[i:i+chunk_size]
        try:
            r = requests.post(data_url, json={"query": gql_query, "variables": {"ids": chunk}})
            entries = r.json()["data"]["entries"]
            
            for entry in entries:
                if not entry: continue
                
                title = entry["struct"]["title"] or ""
                
                # FILTRO: ¬øSuena a bomba de eflujo?
                is_relevant = False
                for k in KEYWORDS:
                    if k.upper() in title.upper():
                        is_relevant = True
                        break
                
                if not is_relevant: continue
                
                # Resoluci√≥n (Si es nula, ponemos 99.9)
                res_list = entry["rcsb_entry_info"]["resolution_combined"]
                res = res_list[0] if res_list else 99.9
                
                candidates.append({
                    "id": entry["rcsb_id"],
                    "res": res,
                    "title": title[:60],
                    "method": entry["rcsb_entry_info"]["structure_determination_methodology"]
                })
        except: continue

    if not candidates:
        print("‚ùå NO SE ENCONTRARON BOMBAS AUTOM√ÅTICAMENTE.")
        # FALLBACK MANUAL: Si todo falla, forzamos la mejor conocida
        winner = {"id": "6OCR", "res": 3.2, "title": "Crystal structure of AdeB (Forzado por sistema)"}
        print(f"‚ö†Ô∏è  ACTIVANDO PROTOCOLO DE EMERGENCIA: Usando {winner['id']} (Est√°ndar de Oro).")
    else:
        # Ordenar y ganar
        df = pd.DataFrame(candidates)
        df = df.sort_values(by="res", ascending=True)
        winner = df.iloc[0]
        
        print("\nüèÜ TOP 3 CANDIDATOS ENCONTRADOS:")
        print(df.head(3).to_string(index=False))

    # 3. ACTUALIZAR CONFIG
    print(f"\n‚öôÔ∏è  Configurando proyecto con: {winner['id']} ({winner['res']} √Ö)")
    
    with open(CONFIG_FILE, 'r') as f:
        config = yaml.safe_load(f)
    
    config['structure']['pdb_id'] = winner['id']
    
    with open(CONFIG_FILE, 'w') as f:
        yaml.dump(config, f, sort_keys=False)
        
    print("‚úÖ LISTO. Configuraci√≥n guardada.")

except Exception as e:
    print(f"Error: {e}")
----------------------------------------
